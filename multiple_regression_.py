# -*- coding: utf-8 -*-
"""Multiple Regression .ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1EKMU7oJtSEgCgzBz_78FudEhW6ocwQ8A
"""

import argparse
import numpy as np
from matplotlib import pyplot as plt

# Compute the sample mean and standard deviations for each feature (column)
# across the training examples (rows) from the data matrix X.
def mean_std(X):
    mean = np.mean(X, axis=0)
    std = np.std(X, axis=0)
    return mean, std

# Standardize the features of the examples in X by subtracting their mean and
# dividing by their standard deviation, as provided in the parameters.
def standardize(X, mean, std):
    S = (X - mean) / std
    return S

# Read data matrix X and labels t from text file.
def read_data(file_name):
    data = np.loadtxt(file_name)
    X = data[:, :-1]
    t = data[:, -1]
    return X, t

# Implement gradient descent algorithm to compute w = [w0, w1, ..].
def train(X, t, eta, epochs):
    costs = []
    ep = []
    w = np.zeros(X.shape[1])
    for epoch in range(epochs):
        gradient = compute_gradient(X, t, w)
        w -= eta * gradient
        if epoch % 10 == 0:
            cost = compute_cost(X, t, w)
            costs.append(cost)
            ep.append(epoch)
    return w, ep, costs

# Compute RMSE on dataset (X, t).
def compute_rmse(X, t, w):
    N = len(t)
    predictions = np.dot(X, w)
    mse = np.mean((predictions - t) ** 2)
    rmse = np.sqrt(mse)
    return rmse

# Compute objective function (cost) on dataset (X, t).
def compute_cost(X, t, w):
    N = len(t)
    predictions = np.dot(X, w)
    error = predictions - t
    cost = (1 / (2 * N)) * np.dot(error.T, error)
    return cost

# Compute gradient of the objective function (cost) on dataset (X, t).
def compute_gradient(X, t, w):
    N = len(t)
    predictions = np.dot(X, w)
    error = predictions - t
    gradient = (1 / N) * np.dot(X.T, error)
    return gradient

# BONUS: Implement stochastic gradient descent algorithm to compute w = [w0, w1, ..].
def train_SGD(X, t, eta, epochs):
    costs = []
    ep = []
    w = np.zeros(X.shape[1])
    for epoch in range(epochs):
        for i in range(len(t)):
            random_index = np.random.randint(0, len(t))
            X_i = X[random_index:random_index+1]
            t_i = t[random_index:random_index+1]
            gradient = compute_gradient(X_i, t_i, w)
            w -= eta * gradient
        if epoch % 10 == 0:
            cost = compute_cost(X, t, w)
            costs.append(cost)
            ep.append(epoch)
    return w, ep, costs

def main():
    # Read the training and test data.
    Xtrain, ttrain = read_data("/content/train.txt")
    Xtest, ttest = read_data("/content/test.txt")

    # Compute mean and standard deviation for feature scaling
    mean, std = mean_std(Xtrain)

    # Standardize the training and test features
    Xtrain = np.hstack((np.ones((Xtrain.shape[0], 1)), standardize(Xtrain, mean, std)))
    Xtest = np.hstack((np.ones((Xtest.shape[0], 1)), standardize(Xtest, mean, std)))

    # Computing parameters for each training method for eta=0.1 and 200 epochs
    eta = 0.1
    epochs = 200

    # Training the model with GD
    w_gd, eph_gd, costs_gd = train(Xtrain, ttrain, eta, epochs)

    # Print model parameters for GD
    print('Params GD: ', w_gd)

    # Print cost and RMSE on training data for GD
    print('Training RMSE GD: %0.2f.' % compute_rmse(Xtrain, ttrain, w_gd))
    print('Training cost GD: %0.2f.' % compute_cost(Xtrain, ttrain, w_gd))

    # Print cost and RMSE on test data for GD
    print('Test RMSE GD: %0.2f.' % compute_rmse(Xtest, ttest, w_gd))
    print('Test cost GD: %0.2f.' % compute_cost(Xtest, ttest, w_gd))

    # Plotting epochs vs. cost for gradient descent methods
    plt.xlabel('Epochs')
    plt.ylabel('Cost')
    plt.yscale('log')
    plt.plot(eph_gd, costs_gd, 'bo-', label='Train J(w) GD')

    # Training the model with SGD
    w_sgd, eph_sgd, costs_sgd = train_SGD(Xtrain, ttrain, eta, epochs)

    # Print model parameters for SGD
    print('Params SGD: ', w_sgd)

    # Print cost and RMSE on training data for SGD
    print('Training RMSE SGD: %0.2f.' % compute_rmse(Xtrain, ttrain, w_sgd))
    print('Training cost SGD: %0.2f.' % compute_cost(Xtrain, ttrain, w_sgd))

    # Print cost and RMSE on test data for SGD
    print('Test RMSE SGD: %0.2f.' % compute_rmse(Xtest, ttest, w_sgd))
    print('Test cost SGD: %0.2f.' % compute_cost(Xtest, ttest, w_sgd))

    # Plotting epochs vs. cost for stochastic gradient descent methods
    plt.plot(eph_sgd, costs_sgd, 'ro-', label='Train J(w) SGD')
    plt.legend()
    #plt.savefig('gd_sgd_cost_comparison.png')
    plt.show()
    plt.close()

if __name__ == "__main__":
    main()